About Least Squares Biases
---------------------------


Least Squares are used everywhere in this PSF modelling code. We
use everywhere hand-made Gauss-Newton solvers: We compute the
gradient (B) and the Hessian (A) of the chi^2 and we solve
brutally AX = B for the new parameters (or generally their
offsets X)

   This may seem simple but there are a certain number of traps related
to biases.

  Generally speaking, if one iterates the fit, the accuracy of A is not
really mandatory, since the minimum is reached when B=0, and in this
case, whatever A is (hopefully non-singular), X=0 is the solution of
AX=B. However an accurate A speeds up convergence.  One should take
care at computing an accurate B, where accurate here means unbiased.
More specifically, for each square in the chi^2 call:
   - r : the residual,
   - h : the derivative of the residual w.r.t the parameters,
   - w : the weight of the square.
In principle, E[r] = 0 and Var(r) = 1/w.

Rhe chi^2 reads sum (r^2 w), and (half) the gradient (B) reads

     B = sum (r*h*w) 

We wish that <B>=0, where the average is taken over realizations of
the data, for the true value of the parameters. This ensures that the
gradient is 0 on average and parameters have a chance to be unbiased.
For the true value of the parameters <r> = 0, but if w or h depend on
the value of the data, then <r*h*w> may be non zero.
 Examples:

   - You are fitting a PSF to some image (of a star). You compute the
   weight from the "observed data" such as the pixel value. Then a
   positive fluctuation leads to reduced weight and <r*w> is non zero.

   - You are trying to figure out non-linearity of the CCD response, and fitting
   a polynomial (of the pixel value) to the residuals of a PSF fit. If you compute
   the derivatives of the polynomial using the observed pixel value, then <r*h> is non-zero
   and you will find a fake non-linearity.

In both of these example, one way to reduce considerably the biases is
to compute the weight and/or the derivatives not from the data, but
from the model (supposedly equal on average). The price to pay is that
it usually introduces non-linearities, even in otherwise linear
problems. The non-linearities are however moderate and much less a
worry than biases. These (potential) biases are serious and iterating
does not cure the problem.  An order of magnitude, for psf photometry
can be found in Sebastien Fabbro's PhD.

About the accuracy of A, the worry is much less serious. A reads

     A = sum (h*transposed(h))

and A is correct on average if h is free from statistical noise. If this
is not the case, biases will anyway arise from B being on zero on average
for the (true) value of the parameters. So trying to get h uncorrelated to
the residal is a good idea if applicable. If not, you have to enter the world of 
"Functional Relationship", a whole field by itself, and worth a few chapters
in Kendall & Start.


Pierre Astier (June 06).
